Audio and full text version is available advertisement free at: https://catpea.com or visit https://github.com/catpea/ for source-code
--------------------------------------------------------------------------------

Teach Me JavaScript; Or, From Local AI To Great Being
Friday • December 26th 2025 • 8:08:50 pm

Preface

This text is written with great urgency,
and in great haste, the same haste with which I installed my local AI.

I want you to know the words,
that have now become part of my daily vocabulary, as I talk to machines.

I want you to know that the world has changed,
and that you must change too.

As person new to programming, you must embrace AI first,
as a seasoned programmer, you have to re-learn programming, with AI.

Parents, this is a matter of great urgency, this is not about money,
don’t think twice about it,

Get the computer, and get someone to help with it at first,
sometimes the first steps are the hardest.

Pay attention, remember that school didn't work for you,
but AI will beigin teaching instantly.

I repeat, this is a matter of great urgency,
schools are sad theater compared to even local AI, [as evidenced][1].

---

Good news is that you can run fast and smart AI at home,
your parents MUST purchase a fast gaming desktop.

The fastest thing you can find,
at the local stores, it wont be $5K, it should be half that.

Your parents must know this is important,
more important than college, because here you learn for real.

You have to install linux, so get a generic desktop computer,
and the firs tthing you do, is use AI, to ask now what.

In 2025, the answer is ollama, becasuse it is simple to use,
but I installed llama.cpp and I had a $20/mo coding assistant.

Modify the llama.cpp source code,
to optimize it for my computer.

llama.cpp is not the fastest,
vLLM seems better, but it is python.

And then nvidia has a TensorRT-LLM,
which requres specific versions of software, and OS.

A rational requirement,
but you very much dedicate your desktop to it.

llama.cpp uses gguf files,
I call them thinking files.

So I download one 10GB file,
and tell llama to use it.

I just want one thing to run well,
you may with to experiment.

I tried LM-Studio, and that looked nice,
I like ComfyUI as their templates are good education.

And ollama has you type in one command in the terminal,
and it launches a model and you can chat with it in the terminal.

I am currently using zed editor for my programming,
and zed does support ollama, so it is a nice combination.

Ollama is a resonable combination,
and I do recommend it.

---...

----- snip ----- (Sorry, 5,000 letter limit in summaries see catpea.com or visit https://github.com/catpea/ for source-code) ----- snip -----

...l AI for weeks now,
AI is unstable.

What I got from properly configured Qwen,
was bad compared to nicely configured GPT-OSS.

A medium Qwen gave me 70 tokens per second,
this means it generates code faster than anyone can read it.

But it didn’t work with crush my replacement for claude,
we don’t know why.

It didn’t listen right, because AI is unstable,
and we have to move fast.

I expect similar problems from GPT-OSS,
but only once in a long while.

However, I only get 30 tokens per second,
it is fast, but, you do not want to go lower.

I use a RTX 5080, ChaGPT tells me I should get 100 tokens,
Claude tells me 35 is top of the line.

In fact, RTX5090 which costs $1000 dollars more,
would not run faster according to claude.

It would just have larger context,
support larger chats, but you don’t need chats that big.

---

Qwen and GPT-OSS,
are AI models.

Models, are the AI, there are many models,
different companies make their own models.

And they release many model variations,
that are good at different things.

Chat, programming, video, images, following instructions,
and general purpose, a little bit of everything.

And then coders, people, optimize those further,
to run faster on cheaper computers.

---

Normal people, who will not pay $10K for a computer,
use optimized models, they are not bad, but amlost not god-like.

If you have an RTX 5080 with 16GB of ram,
you will get 35 token per second with a gpt-oss gguf file under llama.cpp

It is a wonderful start,
you will optimize the heck out of this.

There are better models, already,
but gpt-oss is your start.

--

And once more, I repeat,
do nothing, without the assistance of artificial intelligence.

It is a new way of learning,
it is not as baroque as no AI.

But it is rapid, precisely what you need,
to put up a good fight for your future.

---

AI is a miracle,
“thinking files” should not exist yet.

Fight for it, pay for it, get a local AI to induce learning,
learn programming, end the terror of poverty, and begin rising.

You  deserve so much better than this,
but this is the most powerful method of breaking out there is.

From here you collaborate with your AI,
to create programs, games, tools, services, schools...

[Teach Me JavaScript][1]

[1]: files/teach-me.md
